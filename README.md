## Hi there 👋

My name is Jeonghyeon Kim, and I am a Ph.D. student in Data Science at <a href='https://www.seoultech.ac.kr'>SeoulTech</a>, advised by <a href='https://sites.google.com/ds.seoultech.ac.kr/daintlab/members/director?authuser=0'>Prof. Sangheum Hwang</a> of <a href='https://sites.google.com/ds.seoultech.ac.kr/daintlab/'>DAINTLAB</a>.

 🧗‍♂️I am dedicated to enhancing the reliability and interpretability of AI systems, particularly through:
 
 - Out-of-distribution detection (OoDD)
 - LLM unlearning
 - Energy based models (EBMs)

 💥Ultimate Goal
 
 To develop AI systems that are reliable, and interpretable, ensuring their trustworthy deployment in real-world applications.


<!--
**ma-kjh/ma-kjh** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...

 
 ⭐️Research Focus
 
 My research explores OoDD in vision-language models (VLMs) like CLIP, leveraging multi-modal fine-tuning (MMFT). In our work, we have addressed the modality gap between image and text embeddings through cross-modal alignment (CMA), enhancing the utilization of pretrained knowledge. Moving forward, we aim to extend this research to multi-modal pre-training and integrate large language models.
 
 I am also interested in LLM unlearning, with a particular focus on preserving utility performance while effectively unlearning target knowledge. I have developing strategies for dataset sampling methods that optimize the balance between forgetting targeted information and maintaining model utility.
 
-->
