## Hi there ğŸ‘‹

My name is Jeonghyeon Kim, and I am a Ph.D. student in Data Science at <a href='https://www.seoultech.ac.kr'>SeoulTech</a>, advised by <a href='https://sites.google.com/ds.seoultech.ac.kr/daintlab/members/director?authuser=0'>Prof. Sangheum Hwang</a> of <a href='https://sites.google.com/ds.seoultech.ac.kr/daintlab/'>DAINTLAB</a>.

 ğŸ§—â€â™‚ï¸I am dedicated to enhancing the reliability and interpretability of AI systems, particularly through:
 
 - Out-of-distribution detection (OoDD)
 - LLM unlearning
 - Energy based models (EBMs)

 ğŸ’¥Ultimate Goal
 
 To develop AI systems that are reliable, and interpretable, ensuring their trustworthy deployment in real-world applications.


<!--
**ma-kjh/ma-kjh** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...

 
 â­ï¸Research Focus
 
 My research explores OoDD in vision-language models (VLMs) like CLIP, leveraging multi-modal fine-tuning (MMFT). In our work, we have addressed the modality gap between image and text embeddings through cross-modal alignment (CMA), enhancing the utilization of pretrained knowledge. Moving forward, we aim to extend this research to multi-modal pre-training and integrate large language models.
 
 I am also interested in LLM unlearning, with a particular focus on preserving utility performance while effectively unlearning target knowledge. I have developing strategies for dataset sampling methods that optimize the balance between forgetting targeted information and maintaining model utility.
 
-->
